

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>3. Data, Uncertainty and Random Variables &#8212; Lecture Notes on Fundamental of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture3/00_main';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="2.5. Introduzione a Pandas" href="../lecture2/05_intro_pandas.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../docs/index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../lecture1/00_main.html">1. What is Data Analysis?</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lecture1/01_what_is_data.html">1.1. What is Data? An informal Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture1/02_data_analysis_definition.html">1.2. A Definition of Data Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture1/03_data_analysis_workflow.html">1.3. Data Analysis Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture1/04_examples_data_analysis.html">1.4. Examples of Data Analysis: the Good, the Bad, and the Ugly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture1/05_data_analysis_uses.html">1.5. What’s Data Analysis Used For Today</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture1/06_map_course.html">1.6. Where do We Go From Here?</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lecture2/00_main.html">2. Introduction to Python for Data Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lecture2/01_setup.html">2.1. Installazione dell’ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture2/02_intro_python.html">2.2. Introduzione a Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture2/03_intro_numpy.html">2.3. Introduzione a Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture2/04_intro_matplotlib.html">2.4. Introduzione a Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture2/05_intro_pandas.html">2.5. Introduzione a Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Data, Uncertainty and Random Variables</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/lecture3/00_main.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Data, Uncertainty and Random Variables</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">3. Data, Uncertainty and Random Variables</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#informal-definition-of-data">3.1. Informal Definition of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-importance-of-probability-theory">3.2. Uncertainty &amp; Importance of Probability Theory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variable">3.3. Random Variable</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">3.3.1. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-formal-definition-of-data">3.4. More Formal Definition of Data</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#probability">4. Probability</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-approach-to-probability">4.1. Frequentist Approach to Probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-probability">4.1.1. Example of Probability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-probability">4.2. Joint probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">4.2.1. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-rule-marginal-probability">4.3. Sum Rule (Marginal Probability)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-derivation">4.3.1. Frequentist Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">4.4. Conditional Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#product-rule-factorization">4.5. Product Rule (Factorization)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">4.5.1. Frequentist Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-fundamental-rules-of-probability">4.6. The Fundamental Rules of Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule-of-conditional-probabilities">4.7. The Chain Rule of Conditional Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">4.8. Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approach-to-probability">4.8.1. Bayesian Approach to Probability</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#degree-of-belief">4.8.1.1. Degree of belief</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-probability-manipulation">4.9. Example of Probability Manipulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#excercise">4.9.1. Excercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-conditional-independence">4.10. Independence and Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">4.11. References</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="data-uncertainty-and-random-variables">
<h1><span class="section-number">3. </span>Data, Uncertainty and Random Variables<a class="headerlink" href="#data-uncertainty-and-random-variables" title="Permalink to this heading">#</a></h1>
<section id="informal-definition-of-data">
<h2><span class="section-number">3.1. </span>Informal Definition of Data<a class="headerlink" href="#informal-definition-of-data" title="Permalink to this heading">#</a></h2>
<p>We have seen data social media is “all about the data” and about what we can do with this data. We will start by giving a definition of what we mean by ‘data’.</p>
<p>Following Wikipedia, we could say that:</p>
<p>‘Data is a set of values of subjects<br />
with respect to qualitative or quantitative variables’</p>
<p><u>Qualitative</u>: properties that can be observed and cannot generally be measured with a numerical value (e.g., ‘color’, ‘sex’);</p>
<p><u>Quantitative</u>: properties that can be quantified with a number (e.g., ‘height’, ‘weight’, ‘age’).</p>
<figure class="align-default">
<img alt="../../_images/image1.png" src="../../_images/image1.png" />
</figure>
<p>For example, a twitter post (a ‘tweet’) is data, i.e., a set of data with respect to different variables, such as user, time, text, images, comments, etc.</p>
<p>Some of these entities (e.g., comments) can be in turn a list of entities (e.g., a list of comments).</p>
</section>
<section id="uncertainty-importance-of-probability-theory">
<h2><span class="section-number">3.2. </span>Uncertainty &amp; Importance of Probability Theory<a class="headerlink" href="#uncertainty-importance-of-probability-theory" title="Permalink to this heading">#</a></h2>
<p>We have seen that dealing with data means dealing with some variables assuming some values.</p>
<p>However, what can we say about what values will assume such variables?</p>
<p>In some cases, it is possible to predict the values of variables with perfect accuracy, given a set of initial conditions. For instance, think about a system of equations describing the speed of objects according to Newtonian laws.</p>
<p>In other cases, modeling the relationship between variables in a deterministic way is not possible.</p>
<p>We often say that this is due to <u>uncertainty.</u></p>
<p><strong>Uncertainty in a system can be due to different factors:</strong></p>
<ul class="simple">
<li><p><u>Inherent stochasticity in the system being modeled.</u> For example, most interpretations of quantum mechanics describe the dynamics of subatomic particles as being probabilistic. We can also create theoretical scenarios that we postulate to have random dynamics, such as a hypothetical card game where we assume that the cards are truly shuﬄed into a random order.</p></li>
<li><p><u>Incomplete observability.</u> Even deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system. For example, in the Monty Hall problem, a game show contestant is asked to choose between three doors and wins a prize held behind the chosen door. Two doors lead to a goat while a third leads to a car. The outcome given the contestant’s choice is deterministic, but from the contestant’s point of view, the outcome is uncertain.</p></li>
<li><p><u>Incomplete modeling.</u> When we use a model that must discard some of the information we have observed, the discarded information results in uncertainty in the model’s predictions. For example, suppose we build a robot that can exactly observe the location of every object around it. If the robot discretizes space when predicting the future location of these objects, then the discretization makes the robot immediately become uncertain about the precise position of objects: each object could be anywhere within the discrete cell that it was observed to occupy.</p></li>
</ul>
<p><strong>Examples</strong></p>
<ul class="simple">
<li><p><u>Tossing a coin or rolling a dice</u>: these kind of experiments are generally impossible to model in a deterministic way. This can be due to our limited ability to model the event (i.e., rolling a dice might be deterministic, but deriving a set of equations to determine the outcome might be too hard).</p></li>
<li><p><u>Determining if a patient has a given pathology</u>: different pathologies might have similar symptoms. Hence, observing some of them does not allow to determine with perfect accuracy if the patient has that pathology. In this case, uncertainty might arise from incomplete observability.</p></li>
</ul>
<p><strong>Importance of Probability Theory</strong></p>
<p>Probability theory provides a consistent framework to work with uncertain events.</p>
<p>It allows to quantify and manipulate uncertainty with a set of axioms, as well as to derive new uncertain statements.</p>
<p>Probability theory is hence an important tool to work with data. We will start to revise the main concepts behind probability theory by talking about random variables.</p>
</section>
<section id="random-variable">
<h2><span class="section-number">3.3. </span>Random Variable<a class="headerlink" href="#random-variable" title="Permalink to this heading">#</a></h2>
<p>We discovered that ‘data’ has something to do with ‘variables’. When dealing with uncertain events, we need to use the concept of ‘random variables’. Informally (from wikipedia):</p>
<p>‘A random variable is a variable whose values<br />
depend on outcomes of a random phenomenon.’</p>
<p>A random variable is characterized by a set of possible values often called ‘<u>probability space</u>’ or ‘<u>alphabet</u>’ (this last term comes from information theory, where we often deal with sources emitting symbols from an alphabet, in which case the values of <span class="math notranslate nohighlight">\(X\)</span> will be the symbols).</p>
<p>Random variables can be <u>discrete</u> or <u>continuous</u>. Discrete random variables can assume a finite number (or a countable infinite number) of possible values, whereas a continuous random variable is generally associated with a real number.</p>
<p>A random variable is generally denoted by a <u>capital letter</u>, such as <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Random variables can be <u>scalar</u> (e.g., X=1) or <u>multi-dimensional</u> (e.g., <span class="math notranslate nohighlight">\(X = \binom{1}{3}\)</span>, or <span class="math notranslate nohighlight">\(X = \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
\end{pmatrix}\)</span>).</p>
<p>A random variable is always related to some <u>uncertain phenomenon</u> which generates observations. This is often referred also as ‘experiment’. For instance, if a random variable takes the values of tossing a coin, then ‘tossing a coin’ is the underlying experiment.</p>
<p><strong>Example – discrete scalar</strong></p>
<ul class="simple">
<li><p>For example, <span class="math notranslate nohighlight">\(X\)</span> may denote the outcome of tossing a coin;</p></li>
<li><p>In this context, ‘tossing a coin’ is the random phenomenon characterizing the random variable;</p></li>
<li><p>The space of possible values that <span class="math notranslate nohighlight">\(X\)</span> can assume (the alphabet of <span class="math notranslate nohighlight">\(X\)</span>) can be defined as <span class="math notranslate nohighlight">\(\left\{ head,\ tail \right\}\)</span>. In this example, <span class="math notranslate nohighlight">\(X\)</span> is discrete;</p></li>
<li><p>If tossing the coin, the outcome is <span class="math notranslate nohighlight">\(head\)</span>, then the variable <span class="math notranslate nohighlight">\(X\)</span> assumes the value <span class="math notranslate nohighlight">\(X = head\)</span>.</p></li>
</ul>
<p><strong>Example – continuous scalar</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> may denote the height in meters of a student in this class;</p></li>
<li><p>If we pick a student of heigh 1.75m, then <span class="math notranslate nohighlight">\(X = 1.75\)</span>;</p></li>
</ul>
<p><strong>Example – continuous multi-dimensional</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> may denote the latitude and longitude coordinates of a car in the world;</p></li>
<li><p>Once we pick a car, we may have <span class="math notranslate nohighlight">\(X = \binom{37}{15}\)</span>.</p></li>
</ul>
<section id="example">
<h3><span class="section-number">3.3.1. </span>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<figure class="align-default">
<img alt="../../_images/image2.png" src="../../_images/image2.png" />
</figure>
<p>Let’s assume we have two colored boxes: one red and one blue.</p>
<p>Each box contains two kinds of fruits: apples and oranges.</p>
<p>We consider the experiment of randomly drawing some fruit from one of the two boxes. This happens in two stages:</p>
<ul class="simple">
<li><p>We first randomly pick one of the two boxes;</p></li>
<li><p>Then we randomly pick one of the fruits in the box;</p></li>
<li><p>After observing the type of the fruit, we replace it in the same box.</p></li>
</ul>
<p>The outcome of the experiment can be characterized by <u>two random variables</u>:</p>
<ul>
<li><p>B represents the color of the box and can take values r (red) and b (blue).</p></li>
<li><p>F represents the kind of fruit and can take values a (apple) and o (orange).</p></li>
<li><p>If we pick an orange from the blue box, then the outcome of the experiment can be characterized by the values <span class="math notranslate nohighlight">\(F = o\)</span>, <span class="math notranslate nohighlight">\(B = b\)</span>;</p>
<ol class="arabic">
<li><p>As per our definition of data, the values <span class="math notranslate nohighlight">\(F = o\)</span>, <span class="math notranslate nohighlight">\(B = b\)</span> are ‘data’. Indeed, they represent a ‘bidimensional data point’ and describe the outcome of an experiment.</p>
<p>An alternative view of having two random variables, is to have a single multi-dimensional variable <span class="math notranslate nohighlight">\(X = \lbrack o,b\rbrack\)</span>.</p>
</li>
</ol>
</li>
</ul>
</section>
</section>
<section id="more-formal-definition-of-data">
<h2><span class="section-number">3.4. </span>More Formal Definition of Data<a class="headerlink" href="#more-formal-definition-of-data" title="Permalink to this heading">#</a></h2>
<p>We will define “data” as:</p>
<p>“The values assumed by a random variable”</p>
<p><strong>Example</strong></p>
<ul class="simple">
<li><p>For instance, if the outcome of tossing a coing is <span class="math notranslate nohighlight">\(head\)</span>, then <span class="math notranslate nohighlight">\(X = head\)</span> <u>is data</u>;</p></li>
<li><p>It should be clear that the ‘data’ is <u>the pair</u> &lt;random variable, value&gt; and not just the value. Indeed <span class="math notranslate nohighlight">\(head\)</span> alone would not be very useful (we don’t know which phenomenon it is related to), whereas <span class="math notranslate nohighlight">\(X = head\)</span> can be useful, as we know that <span class="math notranslate nohighlight">\(X\)</span> is the random variable describing the outcome of tossing a coin;</p></li>
<li><p>In this example, the data <span class="math notranslate nohighlight">\(X = head\)</span> is representing a fact: ‘I tossed a coin and the outcome was <span class="math notranslate nohighlight">\(head\)</span>’. This is also called ‘an event’;</p></li>
</ul>
<p><strong>Example</strong></p>
<ul>
<li><p>We can characterize a tweet with the following random variabiles:</p>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(X\)</span>: its textual content;</p>
<p><span class="math notranslate nohighlight">\(Y_{1},\ldots,Y_{n}\)</span>: the attached images;</p>
<p><span class="math notranslate nohighlight">\(Z\)</span>: the date of publication;</p>
</li>
</ol>
</li>
<li><p>Once we pick a tweet, we may have:</p>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(X = &quot;Hello\ world&quot;\)</span>;</p>
<p><span class="math notranslate nohighlight">\(Y_{1} = \begin{pmatrix}
128 &amp; \cdots &amp; 12 \\
 \vdots &amp; \ddots &amp; \vdots \\
33 &amp; \cdots &amp; 65 \\
\end{pmatrix}\)</span> - a matrix representing an attached image;</p>
<p><span class="math notranslate nohighlight">\(Z = 26/09/2019,\ 15:19\)</span></p>
</li>
</ol>
</li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="probability">
<h1><span class="section-number">4. </span>Probability<a class="headerlink" href="#probability" title="Permalink to this heading">#</a></h1>
<p>Since random variables are related to stochastic phenomena, we cannot say much about the outcome of a single phenomenon.</p>
<p>However, we expect to be able to characterize the class of experiments related to a random variable, to infer rules on what values the random variable is likely to take.</p>
<p>For instance, in the case of coin tossing, we can observe that, if I toss a coin for a large number of times, the number of heads will be roughly similar to the number of tails.</p>
<p>This kind of observations is useful, as it can give us a prior on what values we are likely to encounter and what are not.</p>
<p>To formally express such rules, we can define the concept of probability on a random variable.</p>
<p>Specifically, it is possible to assign a probability value to a given outcome. This is generally represented with a capital P:</p>
<ul class="simple">
<li><p>For instance, <span class="math notranslate nohighlight">\(P(B = b)\)</span> represents the probability of picking a blue box in the previous example;</p></li>
<li><p>A probability <span class="math notranslate nohighlight">\(P(B = b)\)</span> is a number comprised between 0 and 1 which quantifies how likely we believe the event to be;</p>
<ul>
<li><p>0 means impossible;</p></li>
<li><p>1 means certain;</p></li>
</ul>
</li>
</ul>
<p>When it is clear from the context which variable we are referring to, the probability can also be expressed simply as:</p>
<div class="math notranslate nohighlight">
\[P(b) = P(B = b)\]</div>
<p>While it is intuitive to understand that a probability quantifies how likely that an event actually happens, we have not yet seen <strong>how to assign probabilities to outcomes</strong>. We will see that there are two main approaches to reason about probabilities: the frequentist approach and the Bayesian approach.</p>
<section id="frequentist-approach-to-probability">
<h2><span class="section-number">4.1. </span>Frequentist Approach to Probability<a class="headerlink" href="#frequentist-approach-to-probability" title="Permalink to this heading">#</a></h2>
<p>There are two main approaches to probability: frequentist and Bayesian.</p>
<p><strong>Frequentist approach</strong>:</p>
<p>Probability theory was initially developed to analyze the frequency of events. For instance, it can be used to study events like drawing a certain hand of cards in a poker game. These events are repeatable and can be dealt with using frequencies. In this sense, when we say that an event has probability <span class="math notranslate nohighlight">\(p\)</span> of occurring, it means that if we repeat the experiment infinitely many times, then a proportion <span class="math notranslate nohighlight">\(p\)</span> of the repetitions would result in that outcome.</p>
<p>According to the frequentist approach, we can estimate probabilities by repeating an experiment for a large number of times and then computing:</p>
<ul class="simple">
<li><p>The number of trials: how many times we performed the experiment;</p></li>
<li><p>The number of favorable outcomes: how many times the outcome of the experiment was favorable.</p></li>
</ul>
<p>The probability is hence obtained by dividing the number of favorable outcomes by the number of trials.</p>
<p>For instance, let’s suppose we want to estimate the probability of obtaining a ‘head’ by tossing a coin. Let’s suppose we toss the coin 1000 times and obtain 499 heads and 501 tails. We can compute the probability of obtaining head as follows:</p>
<ul class="simple">
<li><p>Number of trials: 1000;</p></li>
<li><p>Number of favorable outcomes: 499.</p></li>
</ul>
<p>The probability of obtaining head will be 499/1000=0.49</p>
<p><strong>Examples:</strong></p>
<ul class="simple">
<li><p>The probability of obtaining ‘head’ when tossing a coin is 0.5. We know that because, if we toss a coin for a large number of times, about half of the times, we will obtain ‘head’;</p></li>
<li><p>The probability of picking a red ball from a box with 40 red balls and 60 blue balls is 0.4. We know this because, if we repeat the experiment for a large number of times, we will observe that proportion.</p></li>
</ul>
<section id="example-of-probability">
<h3><span class="section-number">4.1.1. </span>Example of Probability<a class="headerlink" href="#example-of-probability" title="Permalink to this heading">#</a></h3>
<figure class="align-default">
<img alt="../../_images/image3.png" src="../../_images/image3.png" />
</figure>
<p>Suppose we repeat this experiment for many times and observe that:</p>
<ul class="simple">
<li><p>We pick the red box 40% of the times;</p></li>
<li><p>We pick the blue box 60% of the times;</p></li>
<li><p>Once we selected a box, we are equally likely to select any of the fruits contained in it.</p></li>
</ul>
<p>Using a frequentist approach, we can define the probabilities:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(B = b) = \frac{6}{10}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(B = r) = \frac{4}{10}\)</span></p></li>
<li><p>This is done by using the formula:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P(X = x) = \frac{\#\ of\ times\ X = x}{\#\ trials}\)</span></p></li>
</ol>
</li>
</ul>
</section>
</section>
<section id="joint-probability">
<h2><span class="section-number">4.2. </span>Joint probability<a class="headerlink" href="#joint-probability" title="Permalink to this heading">#</a></h2>
<p>We can define <strong>univariate</strong> (= with respect to only one variable) probabilities <span class="math notranslate nohighlight">\(P(B)\)</span> and <span class="math notranslate nohighlight">\(P(F)\)</span> as we have seen in the previous examples.</p>
<p>However, in some cases, it is useful to define probabilities on more than one variable at the time. For instance, we could be interested in studying the probability of picking a given fruit from a given box. In this case, we would be interested in the <strong>joint probability</strong> <span class="math notranslate nohighlight">\(P(B,F)\)</span>.</p>
<p>In general, we can have joint probabilities with arbitrary numbers of variable. For instance, <span class="math notranslate nohighlight">\(P\left( X_{1},X_{2},X_{3},\ldots,X_{n} \right)\)</span>.</p>
<p>Joint probabilities are symmetric, i.e., <span class="math notranslate nohighlight">\(P(X,Y) = P(Y,X)\)</span>.</p>
<p>We should note that, when dealing with multiple unidimensional variables, we can always define a new multi-variate variable comprising all of them:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X = \left\lbrack X_{1},X_{2} \right\rbrack;\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X) = P\left( X_{1},X_{2} \right)\)</span>.</p></li>
</ul>
<section id="id1">
<h3><span class="section-number">4.2.1. </span>Example<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<figure class="align-default">
<img alt="../../_images/image4.png" src="../../_images/image4.png" />
</figure>
<p>We can see the concept of joint probability in the context of the examples of the two boxes;</p>
<p>We have seen how to define the univariate probability <span class="math notranslate nohighlight">\(P(B)\)</span> over the whole probability space of <span class="math notranslate nohighlight">\(B\)</span>;</p>
<p>However, we could be interested in the probability of both variables jointly: <span class="math notranslate nohighlight">\(P(B,F)\)</span>, i.e., the joint probability of B and F.</p>
<p>To ‘measure’ the joint probability, we could repeat the experiment for many times and observe the outcomes.</p>
<p>We could then build a ‘contingency table’ which keeps track of how many times we observed a given combination.</p>
<p>In the slide, we show an example for two general random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> (they could be B and F):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n_{ij}\)</span> represents the number of times we observed <span class="math notranslate nohighlight">\(X = x_{i}\)</span> and <span class="math notranslate nohighlight">\(Y = y_{j}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(c_{i}\)</span> represents the number of times <span class="math notranslate nohighlight">\(X\)</span> assumed the value <span class="math notranslate nohighlight">\(x_{i}\)</span>, independently from the value assumed by <span class="math notranslate nohighlight">\(Y\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(r_{j}\)</span> represents the number of times <span class="math notranslate nohighlight">\(Y\)</span> assumed the value <span class="math notranslate nohighlight">\(y_{j}\)</span>, independently from the value assumed by <span class="math notranslate nohighlight">\(X\)</span>;</p></li>
</ul>
<p>We can measure the joint probability <span class="math notranslate nohighlight">\(P(X = x_{i},Y = y_{j})\)</span> with a frequentist approach using the formula:</p>
<div class="math notranslate nohighlight">
\[P\left( X = x_{i},\ Y = y_{j} \right) = \frac{n_{ij}}{N}\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the total number of trials (i.e., the sum of all the values in the matrix).</p>
<p>Also, we note that we can define the probabilities of X and Y as follow:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P\left( X = x_{i} \right) = \frac{c_{i}}{N}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left( Y = y_{j} \right) = \frac{r_{j}}{N}\)</span></p></li>
</ul>
</section>
</section>
<section id="sum-rule-marginal-probability">
<h2><span class="section-number">4.3. </span>Sum Rule (Marginal Probability)<a class="headerlink" href="#sum-rule-marginal-probability" title="Permalink to this heading">#</a></h2>
<p>Sometimes we can compute probabilities on a set of variables, but we are interested on a probability on a subset of them.</p>
<p>In particular, we have seen that, given the contingency table related to the two variables X and Y, we can estimate both the joint probability <span class="math notranslate nohighlight">\(P(X,Y)\)</span> and the probability of each of the variables: <span class="math notranslate nohighlight">\(P(X)\)</span> and <span class="math notranslate nohighlight">\(P(Y)\)</span>.</p>
<p>To compute these probabilities, we can use the <strong>sum rule of probability</strong>:</p>
<div class="math notranslate nohighlight">
\[P(X = x) = \sum_{y}^{}{P(X = x,Y = y)}\]</div>
<p>The act of computing <span class="math notranslate nohighlight">\(P(X)\)</span> from <span class="math notranslate nohighlight">\(P(X,Y)\)</span> is also known as marginalization and <span class="math notranslate nohighlight">\(P(X)\)</span> is also named <strong>marginal probability</strong>.</p>
<section id="frequentist-derivation">
<h3><span class="section-number">4.3.1. </span>Frequentist Derivation<a class="headerlink" href="#frequentist-derivation" title="Permalink to this heading">#</a></h3>
<figure class="align-default">
<img alt="../../_images/image5.png" src="../../_images/image5.png" />
</figure>
<p>We can see how the sum rule of probability can be derived from the simple example of the contingency table seen before;</p>
<p>Indeed, given the contingency table between variables X and Y, we have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(c_{i} = \sum_{j}^{}n_{ij}\)</span>: the number of trials in which <span class="math notranslate nohighlight">\(X = x_{i}\)</span> is obtained by summing the numbers in the i-th colum;</p></li>
<li><p><span class="math notranslate nohighlight">\(r_{j} = \sum_{i}^{}n_{ij}\)</span>: the number of trials in which <span class="math notranslate nohighlight">\(Y = y_{j}\)</span> is obtained by summing th enumbers in the j-th row;</p></li>
</ul>
<p>Therefore:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P\left( X = x_{i} \right) = \frac{c_{i}}{N} = \frac{\sum_{j}^{}n_{ij}}{N} = \sum_{j}^{}\frac{n_{ij}}{N} = \sum_{j}^{}{P(X = x_{i},\ Y = y_{j})}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P\left( Y = y_{i} \right) = \frac{r_{j}}{N} = \frac{\sum_{i}^{}n_{ij}}{N} = \sum_{i}^{}\frac{n_{ij}}{N} = \sum_{i}^{}{P(X = x_{i},\ Y = y_{j})}\)</span>;</p></li>
</ul>
</section>
</section>
<section id="conditional-probability">
<h2><span class="section-number">4.4. </span>Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this heading">#</a></h2>
<p>In many cases, we are interested in the the probability of some event, given that some other event happened.</p>
<p>This is called <strong>conditional</strong> <strong>probability</strong> and is denoted as<span class="math notranslate nohighlight">\(\ P\left( X = x \middle| Y = y \right)\)</span> and read as ‘P of X=y given that Y=y’. In this context, <span class="math notranslate nohighlight">\(Y = y\)</span> is the condition, and we are interested in studying the probability of X only in the cases in which the condition is verified.</p>
<p>For instance, in the case of the two boxes, we could be interested in <span class="math notranslate nohighlight">\(P(F = a|B = b)\)</span>, that is to say, what is the probability of picking an apple, given that we know that we are drawing from the blue box?</p>
<p>It should be noted that, in general <span class="math notranslate nohighlight">\(P\left( X \middle| Y \right) \neq P(X)\)</span>.</p>
<p>We can define the conditional probability as follows:</p>
<div class="math notranslate nohighlight">
\[P\left( X = x \middle| Y = y \right) = \frac{P(X = x,\ Y = y)}{P(Y = y)}\]</div>
<p>The conditional probability is defined only when <span class="math notranslate nohighlight">\(P(Y = y) &gt; 0\)</span>, that is, we cannot define a probability conditioned on an event that never happens.</p>
<p>In terms of sets, this can be seen as follows:</p>
<div class="math notranslate nohighlight">
\[P\left( A \middle| B \right) = \frac{P(A \cap B)}{P(B)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(P(A \cap B)\)</span> measures the probability that events A and B happen together, whereas <span class="math notranslate nohighlight">\(P(B)\)</span> represents the probability that B happens alone. The probability <span class="math notranslate nohighlight">\(P(A|B)\)</span> measures how likely it is that A happens, knowing that B has happened. We can see this graphically in terms of sets as follows:</p>
<figure class="align-default">
<img alt="../../_images/image6.png" src="../../_images/image6.png" />
</figure>
</section>
<section id="product-rule-factorization">
<h2><span class="section-number">4.5. </span>Product Rule (Factorization)<a class="headerlink" href="#product-rule-factorization" title="Permalink to this heading">#</a></h2>
<p>We can see the definition of conditional probability:</p>
<div class="math notranslate nohighlight">
\[P\left( X = x \middle| Y = y \right) = \frac{P(X = x,\ Y = y)}{P(Y = y)}\]</div>
<p>As follows:</p>
<div class="math notranslate nohighlight">
\[P(X = x,Y = y) = P\left( X = x \middle| Y = y \right)P(Y = y)\]</div>
<p>which is often referred to as <strong>the product rule</strong>.</p>
<p>The product rule allows to compute joint probabilities starting from conditional probabilities and marginal probabilities. This is useful because measuring joint probabilities generally involves creating large tables, whereas conditional and marginal probabilities might be easier to derive.</p>
<p>This operation of expressing a joint probability in terms of two factors is known as <u>factorization</u>.</p>
<section id="id2">
<h3><span class="section-number">4.5.1. </span>Frequentist Derivation<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<figure class="align-default">
<img alt="../../_images/image7.png" src="../../_images/image7.png" />
</figure>
<p>Also in this case, we can derive the product rule from our example;</p>
<p>In particular, the conditional probability <span class="math notranslate nohighlight">\(P(X = x_{i}|Y = y_{j})\)</span> can be computed as follows in a frequentist way:</p>
<div class="math notranslate nohighlight">
\[P\left( X = x_{i} \middle| Y = y_{j} \right) = \frac{n_{ij}}{r_{j}}\]</div>
<p>Since we are restricting our probabilities to the cases in which <span class="math notranslate nohighlight">\(Y = y_{j}\)</span>, the total number of trials to be consider, is the number of trials in which <span class="math notranslate nohighlight">\(Y = y_{j}\)</span>, which is <span class="math notranslate nohighlight">\(r_{j}\)</span>;</p>
<p>We note that:</p>
<div class="math notranslate nohighlight">
\[P\left( X = x_{i},Y = y_{j} \right) = \frac{n_{ij}}{N} = \frac{n_{ij}}{r_{j}} \cdot \frac{r_{j}}{N} = P\left( X = x_{i} \middle| Y = y_{j} \right)P(Y = y_{i})\]</div>
</section>
</section>
<section id="the-fundamental-rules-of-probability">
<h2><span class="section-number">4.6. </span>The Fundamental Rules of Probability<a class="headerlink" href="#the-fundamental-rules-of-probability" title="Permalink to this heading">#</a></h2>
<figure class="align-default">
<img alt="../../_images/image8.png" src="../../_images/image8.png" />
</figure>
<p>To summarize, there are two fundamental rules of probability:</p>
<ul class="simple">
<li><p>Sum rule: allows to perform marginalization;</p></li>
<li><p>Product rule: allows to factorize probability (i.e., breaking them into factors);</p></li>
</ul>
<p>These two rules are the foundations of the framework of probability and allow to reason about random variables and uncertain statements.</p>
<p>All other rules and theorems (including Bayes’ rule) are derived from these two statements.</p>
</section>
<section id="the-chain-rule-of-conditional-probabilities">
<h2><span class="section-number">4.7. </span>The Chain Rule of Conditional Probabilities<a class="headerlink" href="#the-chain-rule-of-conditional-probabilities" title="Permalink to this heading">#</a></h2>
<p>When dealing with multiple variables, the product rule can be applied in an iterative fashion, thus obtaining the ‘chain rule’ of conditional probabilities.</p>
<p>For instance:</p>
<div class="math notranslate nohighlight">
\[P(X,Y,Z) = P\left( X \middle| Y,Z \right)P(Y,Z)\]</div>
<p>Since:</p>
<div class="math notranslate nohighlight">
\[P(Y,Z) = P\left( Y \middle| Z \right)P(Z)\]</div>
<p>We obtain:</p>
<div class="math notranslate nohighlight">
\[P(X,Y,Z) = P\left( X \middle| Y,Z \right)P\left( Y \middle| Z \right)P(Z)\]</div>
<p>Since joint probabilities are symmetric, we could equally obtain:</p>
<div class="math notranslate nohighlight">
\[P(X,Y,Z) = P\left( Z \middle| Y,X \right)P\left( Y \middle| X \right)P(X)\]</div>
<p>This rule can be formalized as follows:</p>
<div class="math notranslate nohighlight">
\[P\left( X_{1},\ldots,\ X_{n} \right) = P\left( X_{1} \right)\prod_{i = 2}^{n}{P(X_{i}|X_{1},\ldots,\ X_{i - 1})}\]</div>
</section>
<section id="bayes-theorem">
<h2><span class="section-number">4.8. </span>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this heading">#</a></h2>
<p>Given two variables A and B, from the product rule, we obtain:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A,B) = P\left( A \middle| B \right)P(B)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(B,A) = P\left( B \middle| A \right)P(A)\)</span></p></li>
</ul>
<p>Since joint probabilities are symmetric, we have:</p>
<div class="math notranslate nohighlight">
\[P(A,B) = P(B,A) \rightarrow P\left( A \middle| B \right)P(B) = P\left( B \middle| A \right)P(A)\]</div>
<p>Which implies:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(P\left( A \middle| B \right) = \frac{P\left( B \middle| A \right)P(A)}{P(B)}\)</span>;</p>
</div></blockquote>
<p>This last expression is known as Bayes’ Theorem (or Bayes’ rule).</p>
<p>Using the sum rule, the denominator can be seen in terms of the quantities appearing in the numerator:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(B) = \sum_{A}^{}{P(}B|A)P(A)\)</span></p></li>
<li><p>We can see the denominator as a normalization constant required to make sure that <span class="math notranslate nohighlight">\(P(A|B)\)</span> is a valid probability.</p></li>
</ul>
<p>The Bayes’ rule can be used to ‘invert’ conditional proabilities, i.e., to turn probabilities of the kind <span class="math notranslate nohighlight">\(P(A|B)\)</span> into probabilities of the kind <span class="math notranslate nohighlight">\(P(B|A)\)</span>;</p>
<p>The different terms on the Bayes’ rule have names:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A)\)</span> is called ‘the prior’;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|A)\)</span> is called ‘the likelihood’;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B)\)</span> is called ‘the evidence’;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A|B)\)</span> is called ‘the posterior’;</p></li>
<li><p>We will get back to these names with examples in the following slides.</p></li>
</ul>
<section id="bayesian-approach-to-probability">
<h3><span class="section-number">4.8.1. </span>Bayesian Approach to Probability<a class="headerlink" href="#bayesian-approach-to-probability" title="Permalink to this heading">#</a></h3>
<p>Some events are not related to repeatable experiments, but they are still characterized by uncertainty. Examples of such events are ‘will the sun extinguish in 4.5 billion years?’ or ‘given the observation of some symptoms, does the patient have a given pathology’. These events are not related to repeatable experiments. Yet, they are characterized by uncertainty. Also, observing more data might help reduce uncertainty. For instance, if a new symptom is observed, we might be less uncertain about one outcome or another.</p>
<p>**<br />
Examples:**</p>
<ul class="simple">
<li><p>The probability of a patient to have a flu given the observation of their temperature;</p></li>
<li><p>The probability that the average temperature of the planet will increase by 1° next year if we reduce the emission of CO2 by a given amount</p></li>
</ul>
<section id="degree-of-belief">
<h4><span class="section-number">4.8.1.1. </span>Degree of belief<a class="headerlink" href="#degree-of-belief" title="Permalink to this heading">#</a></h4>
<p>An important concept related to Bayesian probability is the one of “degree of belief”. Since the experiments are not repeatable, to quantify probability we cannot count the number of favorable outcomes. Instead, we ‘inject’ our belief into the system. Specifically, this is done using the Bayes’ rule, which provide the tools to insert our degree of belief in a consistent way. We will talk about the Bayes’ rule later.</p>
<p><strong>Example:</strong></p>
<p>Let’s suppose we toss a coin three times and we obtain three heads. Now, three is not a large number and hence, if we try to apply the frequentist approach, we wrongly estimate that the probability of getting head is 1, while the probability of getting tail is zero. By using the Bayesian approach, we can insert our belief that the coin is fair by introducing a “prior probability” that the probability of getting head is 0.5.</p>
</section>
</section>
</section>
<section id="example-of-probability-manipulation">
<h2><span class="section-number">4.9. </span>Example of Probability Manipulation<a class="headerlink" href="#example-of-probability-manipulation" title="Permalink to this heading">#</a></h2>
<figure class="align-default">
<img alt="../../_images/image9.png" src="../../_images/image9.png" />
</figure>
<p>Let’s get back to our example of the two boxes and let’s suppose that, by repeating several trials, we discovered the following probabilities (in a frequentist way):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(B = r) = \frac{4}{10}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(B = b) = \frac{6}{10}\)</span></p></li>
</ul>
<p>We also focused on a given box and performed different trials, observing the following proportions:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P\left( F = a \middle| B = r \right) = \frac{1}{4}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left( F = o \middle| B = r \right) = \frac{3}{4}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left( F = a \middle| B = b \right) = \frac{3}{4}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left( F = o \middle| B = b \right) = \frac{1}{4}\)</span></p></li>
</ul>
<p>We shall note that these probabilities are normalized such that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(B = r) + P(B = b) = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left( F = a \middle| B = r \right) + P\left( F = o \middle| B = r \right) = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P\left( F = a \middle| B = b \right) + P\left( F = o \middle| B = b \right) = 1\)</span></p></li>
</ul>
<p>We can now use the rules we have seen before to answer questions such as:</p>
<ol class="arabic">
<li><p>What is the overall probability of choosing an apple?</p>
<p>What is the probability of picking a red box, given that we have drawn an orange?</p>
</li>
</ol>
<p><strong>What is the overall probability of choosing an apple?</strong></p>
<p>To answer this question, we need to find <span class="math notranslate nohighlight">\(P(F = a)\)</span>. We note that, by the sum rule:</p>
<div class="math notranslate nohighlight">
\[P(F = a) = P(F = a,B = b) + P(F = a,B = r)\]</div>
<p>We also observe that the joint probabilities can be recovered using the product rule:</p>
<div class="math notranslate nohighlight">
\[P(F = a,B = b) = P\left( F = a \middle| B = b \right)P(B = b)\]</div>
<div class="math notranslate nohighlight">
\[P(F = a,B = r) = P\left( F = a \middle| B = r \right)P(B = r)\]</div>
<p>Hence, our probability can be found using the formula:</p>
<div class="math notranslate nohighlight">
\[P(F = a) = P\left( F = a \middle| B = b \right)P(B = b) + P\left( F = a \middle| B = r \right)P(B = r) =\]</div>
<div class="math notranslate nohighlight">
\[= \frac{3}{4} \cdot \frac{6}{10} + \frac{1}{4} \cdot \frac{4}{10} = \frac{18 + 4}{40} = \frac{22}{40} = \frac{11}{20}\]</div>
<p>From the definition of probability, we have:</p>
<div class="math notranslate nohighlight">
\[P(F = a) + P(F = o) = 1\]</div>
<p>Hence:</p>
<div class="math notranslate nohighlight">
\[P(F = o) = 1 - P(F = a) = 1 - \frac{11}{20} = \frac{9}{20}\]</div>
<p><strong>What is the probability of picking a red box, given that we have drawn an orange?</strong></p>
<p>To answer this question, we need to find the conditional probability <span class="math notranslate nohighlight">\(P(B = r|F = o)\)</span>. To find this probability, we need to ‘invert’ our conditional probabilities using the Bayes’ rule:</p>
<div class="math notranslate nohighlight">
\[P\left( B = r \middle| F = o \right) = \frac{P\left( F = o \middle| B = r \right)P(B = r)}{P(F = o)} = \frac{\frac{3}{4} \cdot \frac{4}{10}}{\frac{9}{20}} = \frac{12}{40} \cdot \frac{20}{9} = \frac{6}{9} = \frac{2}{3}\]</div>
<p><strong>Interpretation of Bayes’ rule:</strong></p>
<p>We can give the following interpretation of Bayes’ rule. If somebody asked us what is probability of picking the red box, we would have said it is <span class="math notranslate nohighlight">\(P(B = r) = \frac{4}{10}\)</span>. We call this ‘prior probability’ as it is our <strong>best guess given that no additional constrain is provided</strong>. Once we are told that we have picked an orange, our <strong>belief</strong> on the identity of the box can be refined. In particular, we know that the red box contains more oranges than the blue one, so we should assign probabilities accordingly. We use the Bayes’ theorem to obtain the conditional probability <span class="math notranslate nohighlight">\(P(B = r|F = o)\)</span> as done above. We call this the ‘posterior probability’, as this is our updated estimate, once we know the identity of the fruit. To refine our guess, we use the ‘likelihood’ <span class="math notranslate nohighlight">\(P(F = o|B = r)\)</span> which tells us how many oranges are in the red box and the ‘evidence’ <span class="math notranslate nohighlight">\(P(F = o)\)</span> which tells us how many oranges there are at all.</p>
<section id="excercise">
<h3><span class="section-number">4.9.1. </span>Excercise<a class="headerlink" href="#excercise" title="Permalink to this heading">#</a></h3>
<p>Suppose that we have three colored boxes r (red), b (blue), and g (green). Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange, and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes. If a box is chosen at random with probabilities P(r)=0.2, P(b)=0.2, P(g)=0.6, and a piece of fruit is extracted from the box (with equal probability of selecting any of the items in the box), then what is the probability of selecting an apple? If we observe that the selected fruit is in fact an orange, what is the probability that it came from the green box?</p>
</section>
</section>
<section id="independence-and-conditional-independence">
<h2><span class="section-number">4.10. </span>Independence and Conditional Independence<a class="headerlink" href="#independence-and-conditional-independence" title="Permalink to this heading">#</a></h2>
<p><strong>Independence</strong></p>
<p>Two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>said to be independent</strong> if and only if:</p>
<div class="math notranslate nohighlight">
\[P(X,Y) = P(X)P(Y)\]</div>
<p>It should be noted that the expression above <strong>is generally not true</strong> as we cannot always assume that two variables are independent.</p>
<p>Independence can be denoted as:</p>
<div class="math notranslate nohighlight">
\[X\bot Y\]</div>
<p><strong>Examples</strong></p>
<p>Intuitively, two variables are independent if the values of one of them do not affect the values of the other one:</p>
<ul class="simple">
<li><p>Weight and height of a person are <strong>not independent</strong>. Indeed, taller people are usually heavier;</p></li>
<li><p>Height and richness are <strong>independent</strong>, as the richness does not depend on the height of a person;</p></li>
</ul>
<p><strong>Conditional independence</strong></p>
<p>Two random variales <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are said to be <strong>conditionally independent</strong> given a random variable <span class="math notranslate nohighlight">\(Z\)</span> if:</p>
<div class="math notranslate nohighlight">
\[P\left( X,Y \middle| Z \right) = P\left( X \middle| Z \right)P\left( Y \middle| Z \right)\]</div>
<p>Conditional independence can be denoted as:</p>
<div class="math notranslate nohighlight">
\[X\bot Y\ |\ Z\]</div>
<p><strong>Example</strong></p>
<ul class="simple">
<li><p>Height and vocabulary are not independent: taller people are usually older, and hence they have a more sophisticated vocabulary (they know more words). However, if we condition on age, they become independent. Indeed, among people of the same age, height should not influence vocabulary. Hence, height and vocabulary are <strong>conditionally independent</strong> with respect to age;</p></li>
</ul>
</section>
<section id="references">
<h2><span class="section-number">4.11. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Parts of chapter 1 of [1];</p></li>
<li><p>Most of chapter 3 of [2];</p></li>
<li><p>Some parts of chapter 1 of [3].</p></li>
</ul>
<p>[1] Bishop, Christopher M. <em>Pattern recognition and machine learning</em>. springer, 2006. <a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
<p>[2] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep learning</em>. MIT press, 2016. <a class="reference external" href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
<p>[3] Cover, Thomas M., and Joy A. Thomas. <em>Elements of information theory</em>. John Wiley &amp; Sons, 2012.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/lecture3"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../lecture2/05_intro_pandas.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2.5. </span>Introduzione a Pandas</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">3. Data, Uncertainty and Random Variables</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#informal-definition-of-data">3.1. Informal Definition of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-importance-of-probability-theory">3.2. Uncertainty &amp; Importance of Probability Theory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variable">3.3. Random Variable</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">3.3.1. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-formal-definition-of-data">3.4. More Formal Definition of Data</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#probability">4. Probability</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-approach-to-probability">4.1. Frequentist Approach to Probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-probability">4.1.1. Example of Probability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-probability">4.2. Joint probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">4.2.1. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-rule-marginal-probability">4.3. Sum Rule (Marginal Probability)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-derivation">4.3.1. Frequentist Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">4.4. Conditional Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#product-rule-factorization">4.5. Product Rule (Factorization)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">4.5.1. Frequentist Derivation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-fundamental-rules-of-probability">4.6. The Fundamental Rules of Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule-of-conditional-probabilities">4.7. The Chain Rule of Conditional Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">4.8. Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approach-to-probability">4.8.1. Bayesian Approach to Probability</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#degree-of-belief">4.8.1.1. Degree of belief</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-probability-manipulation">4.9. Example of Probability Manipulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#excercise">4.9.1. Excercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-conditional-independence">4.10. Independence and Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">4.11. References</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>