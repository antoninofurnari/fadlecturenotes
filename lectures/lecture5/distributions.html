

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Probability Distributions &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/lecture5/distributions';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_intro_data_analysis.html">1. Introduction to Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_main_data_analysis_concepts.html">2. Main data analysis concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_misure_di_frequenze_e_rappresentazione_grafica_dei_dati.html">3. Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_misure_di_tendenza_centrale_dispersione_e_forma.html">4. Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_associazione_variabili.html">5. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratories</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/01_setup.html">6. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/02_intro_python.html">7. Introduzione a Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/03_intro_numpy.html">8. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/04_intro_matplotlib.html">9. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/05_intro_pandas.html">10. Introduzione a Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/06_misure_di_frequenze_e_rappresentazioni_grafiche_dei_dati.html">11. Laboratorio su Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/07_misure_di_tendenza_centrale_dispersione_e_forma.html">12. Laboratorio su Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../laboratories/08_associazione_variabili.html">13. Associazione tra Variabili</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/lecture5/distributions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/lecture5/distributions.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probability Distributions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Probability Distributions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmf-discrete-variables">Probability Mass Functions (PMF) – Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-fair-coin">Example: Probability Mass Function for a Fair Coin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-skewed-coin">Example: Probability Mass Function for a Skewed Coin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-probability-mass-function">Exercise: Probability Mass Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdf-continuous-variables">Probability Density Functions (PDF) – Continuous Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-uniform-pdf">Example: Uniform PDF</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-distributions">Common Probability Distributions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">Bernoulli Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">Binomial Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">Categorical Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-distribution">Multinomial Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution">Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">Multivariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-gaussian-distribution">Estimation of the Parameters of a Gaussian Distribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theory">Information Theory</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-information">Self-information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#questions">Questions</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="probability-distributions">
<h1>Probability Distributions<a class="headerlink" href="#probability-distributions" title="Permalink to this heading">#</a></h1>
<p>We have seen how it is possible to assign a probability value to a given
outcome of a random variable.</p>
<p>In practice, it is often useful to assign probability values to <strong>all
the values</strong> that the random variable can assume.</p>
<p>To do so, we can define a <strong>function</strong>, which we will call <strong>probability
distribution</strong> which assigns a probability value to each of the possible
values of a random variable.</p>
<p>In the case of discrete variables, we will talk about “<strong>probability
mass functions</strong>”, whereas in the case of continuous variable, we will
refer to “<strong>probability density functions</strong>”.</p>
<p>A probability distribution characterizes the random variable and defines
which outcomes it is more likely to observe.</p>
<p>Once we find that a given random variable <span class="math notranslate nohighlight">\(X\)</span> is characterized by a
probability distirbution <span class="math notranslate nohighlight">\(P(X)\)</span>, we can say that <strong>“X follows P”</strong> and
write:</p>
<div class="math notranslate nohighlight">
\[X \sim P\]</div>
<section id="probability-mass-functions-pmf-discrete-variables">
<h2>Probability Mass Functions (PMF) – Discrete Variables<a class="headerlink" href="#probability-mass-functions-pmf-discrete-variables" title="Permalink to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is discrete, <span class="math notranslate nohighlight">\(P(X)\)</span> is called a “probability mass function”
(PMF). <span class="math notranslate nohighlight">\(P\)</span> maps the values of <span class="math notranslate nohighlight">\(X\)</span> to real numbers indicating whether a
given value is more or less likely.</p>
<p>A PMF on a random variable <span class="math notranslate nohighlight">\(X\)</span> is a function</p>
<div class="math notranslate nohighlight">
\[P:\chi \rightarrow \lbrack 0,1\rbrack\]</div>
<p>Where <span class="math notranslate nohighlight">\(\chi\)</span> is the alphabet of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>This function satisfies the following properties:</p>
<ul class="simple">
<li><p><strong>The domain of</strong> <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> <strong>is the set of all possible states
of</strong> <span class="math notranslate nohighlight">\(\mathbf{X}\)</span><strong>.</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{\forall}\mathbf{x}\mathbf{\in}\chi\mathbf{,\ }\mathbf{0}\mathbf{\leq}\mathbf{P}\left( \mathbf{x} \right)\mathbf{\leq}\mathbf{1}\)</span>.
An impossible event has probability 0, whereas a certain event has
probability 1. No event can have negative probability or probability
larger than 1.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{\mathbf{x}\mathbf{\in}\mathbf{\chi}}^{}{\mathbf{P}\mathbf{(}\mathbf{x}\mathbf{)}}\mathbf{=}\mathbf{1}\)</span>.
The sum of the probabilities associated to all possible events must
be one. This implies that the probability distribution is
normalized. Also, this means that at least one of the events should
happen.</p></li>
</ul>
<p><strong>Example:</strong> Let <span class="math notranslate nohighlight">\(X\)</span> be the random variable indicating the outcome of a
coin toss.</p>
<ul class="simple">
<li><p>The space of all possible functions (the domain of <span class="math notranslate nohighlight">\(P(X)\)</span>) is
<span class="math notranslate nohighlight">\(\{ head,\ tail\}\)</span>.</p></li>
<li><p>The probabilities <span class="math notranslate nohighlight">\(P(head)\)</span> and <span class="math notranslate nohighlight">\(P(tail)\)</span> must be larger than or
equal to zero and smaller than or equal to 1.</p></li>
<li><p>Also, <span class="math notranslate nohighlight">\(P(head) + P(tail) = 1\ \)</span>. This is obvious, as one of the two
outcomes will always happen. Indeed, if we had <span class="math notranslate nohighlight">\(P(tail) = 0.3\)</span>, this
would mean that, 30 times out of 100 times we toss a coin, the
outcome will be tail. What will happen in all other cases? The
outcome will be head, hence, <span class="math notranslate nohighlight">\(P(head)\)</span>, so <span class="math notranslate nohighlight">\(P(head) + P(tail) = 1\)</span>.</p></li>
<li><p>In the case of a fair coin, we can characterize <span class="math notranslate nohighlight">\(P(X)\)</span> as a
“discrete uniform distribution”, i.e., a distribution which maps any
value <span class="math notranslate nohighlight">\(x \in X\)</span> to a constant, such that the properties of the
probability mass functions are satisfied.</p></li>
<li><p>If we have <span class="math notranslate nohighlight">\(N\)</span> possible outcomes, the discrete uniform probability
will be <span class="math notranslate nohighlight">\(P(X = x) = \frac{1}{N}\)</span> , which means that all outcomes
have the same probability.</p></li>
<li><p>This definition satisfies the constraints. Indeed,
<span class="math notranslate nohighlight">\(\frac{1}{N} \geq 0,\ \forall N\)</span> and
<span class="math notranslate nohighlight">\(\sum_{i}^{}{P\left( X = x_{i} \right)} = 1\)</span>.</p></li>
</ul>
<section id="example-probability-mass-function-for-a-fair-coin">
<h3>Example: Probability Mass Function for a Fair Coin<a class="headerlink" href="#example-probability-mass-function-for-a-fair-coin" title="Permalink to this heading">#</a></h3>
<p>A probability mass function can be plotted as a 2D diagram where the
values of the function (<span class="math notranslate nohighlight">\(P(x)\)</span>) is plotted against the values of the
independent variable <span class="math notranslate nohighlight">\(x\)</span>. This is the diagram associated to the PMF of
the previous example, where <span class="math notranslate nohighlight">\(P(head) = P(tail) = 0.5\)</span>.</p>
<p><img alt="" src="lectures/lecture5/media/image1.png" />{width=”6.6930555555555555in”
height=”4.461805555555555in”}</p>
</section>
<section id="example-probability-mass-function-for-a-skewed-coin">
<h3>Example: Probability Mass Function for a Skewed Coin<a class="headerlink" href="#example-probability-mass-function-for-a-skewed-coin" title="Permalink to this heading">#</a></h3>
<p>Now suppose we tossed our coin for 10000 times and discovered that 6000
times the outcome was “head”, whereas 4000 times it was “tail”. We
deduce the coin is not fair.</p>
<p>Using a <strong>frequentist</strong> approach, we can manually assign values to our
PMF using the general formula:</p>
<div class="math notranslate nohighlight">
\[P(x) = \frac{\# trials\ in\ which\ X = x}{\#\ trials}\]</div>
<p>That is, in our case:</p>
<div class="math notranslate nohighlight">
\[P(head) = \frac{6000}{10000} = 0.6;P(tail) = \frac{4000}{10000} = 0.4\]</div>
<p>We shall note that the probability we just defined satisfies all
properties of probabilities, i.e.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(0 \leq P(x) \leq 1\ \forall x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{x}^{}{P(x) = 1}.\)</span></p></li>
</ul>
<p><img alt="" src="lectures/lecture5/media/image2.png" />{width=”6.6930555555555555in” height=”4.4625in”}</p>
</section>
<section id="exercise-probability-mass-function">
<h3>Exercise: Probability Mass Function<a class="headerlink" href="#exercise-probability-mass-function" title="Permalink to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable representing the outcome of rolling a fair
dice with <span class="math notranslate nohighlight">\(6\)</span> faces:</p>
<ul class="simple">
<li><p>What is the space of possible values of <span class="math notranslate nohighlight">\(X\)</span>?</p></li>
<li><p>What is its cardinality?</p></li>
<li><p>What is the associated probability mass function <span class="math notranslate nohighlight">\(P(X)\)</span>?</p></li>
<li><p>Suppose the dice is not fair and <span class="math notranslate nohighlight">\(P(X = 1) = 0.2\)</span>, whereas all other
outcomes are equally probable. What is the probability mass function
of <span class="math notranslate nohighlight">\(P(X)\)</span>?</p></li>
<li><p>Draw the PMF obtained for the dice.</p></li>
</ul>
</section>
</section>
<section id="probability-density-functions-pdf-continuous-variables">
<h2>Probability Density Functions (PDF) – Continuous Variables<a class="headerlink" href="#probability-density-functions-pdf-continuous-variables" title="Permalink to this heading">#</a></h2>
<p>Probability distributions are called “probability density functions”
when the random variable is continuous.</p>
<p>To be a probability density function over a variable <span class="math notranslate nohighlight">\(X\)</span>, a function
<span class="math notranslate nohighlight">\(P:\chi \rightarrow \lbrack 0,1\rbrack\)</span> must satisfy the following
properties:</p>
<ul class="simple">
<li><p>The domain of P is the set of all possible values of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\forall x \in \chi,\ P(x) \geq 0\)</span>. No probability can be negative.</p></li>
<li><p><span class="math notranslate nohighlight">\(\int P(x)dx = 1\)</span>. This is equivalent to <span class="math notranslate nohighlight">\(\sum P(x) = 1\ \)</span>in the
case of discrete variable. The sum turns into an integral in the
case of continuous variables.</p></li>
</ul>
<p><strong>Example</strong></p>
<ul class="simple">
<li><p>Let us consider a random number generator which outputs numbers
comprised between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable assuming the values generated by the
random number generator.</p></li>
<li><p>The PDF of <span class="math notranslate nohighlight">\(X\)</span> will be a uniform distribution such that:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(x) = 0\ \forall x &lt; a\ \ or\ x &gt; b\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(x) = \frac{1}{b - a}\ \forall a \leq x \leq b\)</span>;</p></li>
</ul>
</li>
</ul>
<p>We can see that this PDF satisfies all constraints:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x) \geq 0\ \forall x.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\int P(x)dx = 1\)</span> (prove that this is true as an exercise).</p></li>
</ul>
<section id="example-uniform-pdf">
<h3>Example: Uniform PDF<a class="headerlink" href="#example-uniform-pdf" title="Permalink to this heading">#</a></h3>
<p>The diagram shows an illustration of a uniform PDF with bounds a and b,
i.e., <span class="math notranslate nohighlight">\(U(a,b)\)</span>. Of course, continuous distributions can be (and
generally are) much more complicated than that.</p>
<p><img alt="Risultati immagini per uniform probability densityfunction" src="lectures/lecture5/media/image3.png" />{width=”4.475in”
height=”2.8568875765529307in”}</p>
</section>
</section>
<section id="expectation">
<h2>Expectation<a class="headerlink" href="#expectation" title="Permalink to this heading">#</a></h2>
<p>When it is known that a random variable follows a probability
distribution, <strong>it is possible to characterize that variable</strong> (and
hence the related probability distribution) <strong>with some statistics</strong>.</p>
<p>The most straightforward of them is the expectation. <strong>The concept of
expectation is very related to the concept of mean.</strong> When we compute
the mean of a given set of numbers, we usually sum all the numbers
together and then divide by the total.</p>
<p>Since a probability distribution will tell us which values will be more
frequent than others, we can compute this mean with a weighted average,
where the weights are given by the probability distribution.</p>
<p>Specifically, we can define the expectation of a random variable X as
follows:</p>
<div class="math notranslate nohighlight">
\[E_{X\sim P}\lbrack X\rbrack = \sum_{x \in \chi}^{}{xP(x)}\]</div>
<p>In the case of continuous variables, the expectation takes the form of
an integral:</p>
<div class="math notranslate nohighlight">
\[E_{X \sim P}\lbrack X\rbrack = \int xP(x)dx\]</div>
<p>This is very related to the concept of mean value (or expected value) of
a random variable.</p>
</section>
<section id="variance">
<h2>Variance<a class="headerlink" href="#variance" title="Permalink to this heading">#</a></h2>
<p>The variance gives a measure of how much variability there is in a
variable <span class="math notranslate nohighlight">\(X\)</span> around its mean <span class="math notranslate nohighlight">\(E\lbrack X\rbrack\)</span>.</p>
<p>The variance is defined as follows:</p>
<div class="math notranslate nohighlight">
\[var\lbrack X\rbrack = E\lbrack\left( X - E\lbrack X\rbrack \right)^{2}\rbrack\]</div>
</section>
<section id="covariance">
<h2>Covariance<a class="headerlink" href="#covariance" title="Permalink to this heading">#</a></h2>
<p>The covariance gives a measure of how two variables are linearly related
to each other. It allows to <strong>measure to what extent the increase of one
of the variables corresponds to an increase of the value of the other
one</strong>.</p>
<p>Given two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, the covariance is defined as
follows:</p>
<div class="math notranslate nohighlight">
\[Cov(X,Y) = E\lbrack\left( X - E\lbrack X\rbrack \right)\left( Y - E\lbrack Y\rbrack \right)\rbrack\]</div>
<p>We can distinguish the following terms:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\lbrack X\rbrack\)</span> and <span class="math notranslate nohighlight">\(E\lbrack Y\rbrack\)</span> are the expectations of
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((X - E\lbrack X\rbrack)\)</span> and <span class="math notranslate nohighlight">\((Y - E\lbrack Y\rbrack)\)</span> are the
differences between the samples and the expected values.</p></li>
<li><p><span class="math notranslate nohighlight">\(\left( X - E\lbrack X\rbrack \right)\left( Y - E\lbrack Y\rbrack \right)\)</span>
computes the product between the differences.</p>
<ul>
<li><p>If the signs of the terms agree, the product is positive.</p></li>
<li><p>If the signs of the terms disagree, the product is negative.</p>
<ol class="arabic simple">
<li><p>In practice, if when <span class="math notranslate nohighlight">\(X\)</span> is larger than the mean, then <span class="math notranslate nohighlight">\(Y\)</span>
is larger than the mean and vice versa, when <span class="math notranslate nohighlight">\(X\ \)</span>is lower
than the mean then <span class="math notranslate nohighlight">\(Y\)</span> is lower than the mean, then the two
variables are <em>correlated,</em> and the covariance is high.</p></li>
</ol>
</li>
</ul>
</li>
</ul>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a multi-dimensional variable
<span class="math notranslate nohighlight">\(X = \lbrack X_{1},X_{2},\ldots,X_{n}\rbrack\)</span>, we can compute all the
possible covariances between variable pairs:
<span class="math notranslate nohighlight">\(Cov\lbrack X_{i},X_{j}\rbrack\)</span>. This allows to create a matrix, which
is generally referred to as <strong>the covariance matrix</strong>. The general term
of the covariance matrix <span class="math notranslate nohighlight">\(Cov(X)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[Cov(X)_{i,j} = Cov(X_{i},X_{j})\]</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="common-probability-distributions">
<h1>Common Probability Distributions<a class="headerlink" href="#common-probability-distributions" title="Permalink to this heading">#</a></h1>
<p>There are several common probability distributions which can be used to
describe random events. <strong>These distributions have an analytical
formulation which depends generally on one or more parameters.</strong></p>
<p>When we have <strong>enough evidence that a given random variable is well
described by one of these distributions</strong>, we can simply “fit” the
distribution to the data (i.e., choose the correct parameters for the
distribution) and use the analytical formulation to deal with the random
variable.</p>
<p>It is hence useful to know the <strong>most common probability distributions</strong>
so that we can recognize the cases in which they can be used.</p>
<section id="bernoulli-distribution">
<h2>Bernoulli Distribution<a class="headerlink" href="#bernoulli-distribution" title="Permalink to this heading">#</a></h2>
<p>The Bernoulli distribution is a distribution over a single binary random
variable, i.e., the variable <span class="math notranslate nohighlight">\(X\)</span> can take only two values:
<span class="math notranslate nohighlight">\(\left\{ 0,1 \right\}\)</span>.</p>
<p>The distribution is controlled by a single parameter
<span class="math notranslate nohighlight">\(\phi \in \lbrack 0,1\rbrack\)</span>, which gives the probability of the
variable to be equal to 1.</p>
<p>The analytical formulation of the Bernoulli distribution is very simple:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(X = 1) = \phi\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(X = 0) = 1 - \phi\)</span></p></li>
</ul>
<p>The expected value and variance of the associated random variable are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\lbrack x\rbrack = \phi\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(Var(x) = \phi(1 - \phi)\)</span>.</p></li>
</ul>
<p><strong>Example</strong></p>
<p>The skewed coin in the previous example landed on “head” <span class="math notranslate nohighlight">\(60\%\)</span> of the
times. If we define <span class="math notranslate nohighlight">\(X = 1\)</span> when the outcome is head and <span class="math notranslate nohighlight">\(X = 0\)</span> when
the outcome is tail, then the variable follows a Bernoulli distribution
with <span class="math notranslate nohighlight">\(\phi = 0.6\)</span>.</p>
</section>
<section id="binomial-distribution">
<h2>Binomial Distribution<a class="headerlink" href="#binomial-distribution" title="Permalink to this heading">#</a></h2>
<p><img alt="" src="lectures/lecture5/media/image4.png" />{width=”6.6930555555555555in”
height=”4.461805555555555in”}</p>
<p>The binomial distribution is a discrete probability distribution (PMF**)
over natural numbers with parameters** <span class="math notranslate nohighlight">\(\mathbf{n}\)</span> <strong>and</strong>
<span class="math notranslate nohighlight">\(\mathbf{p}\)</span><strong>;</strong></p>
<p>It models <strong>the probability of obtaining</strong> <span class="math notranslate nohighlight">\(\mathbf{k}\)</span> <strong>successes in a
sequence of</strong> <span class="math notranslate nohighlight">\(\mathbf{n}\)</span> <strong>independent experiments which follow a
Bernoulli distribution with parameter</strong>
<span class="math notranslate nohighlight">\(\mathbf{p}\mathbf{\ (}\mathbf{\phi}\mathbf{=}\mathbf{p}\mathbf{)}\)</span><strong>;</strong></p>
<p>The probability mass function of the distribution is given by:</p>
<div class="math notranslate nohighlight">
\[P(k) = \binom{n}{k}p^{k}(1 - p)^{n - k}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the number of successes</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of independent trials</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> is the probability of a success in a single trial</p></li>
</ul>
<p>The expected value is <span class="math notranslate nohighlight">\(E\lbrack x\rbrack = np\)</span>;</p>
<p>The variance is <span class="math notranslate nohighlight">\(Var\lbrack k\rbrack = np(1 - p)\)</span>;</p>
<p><strong>Example</strong></p>
<p>What is the probability of tossing a coin three times and obtaining
three heads? We have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k = 3\)</span>: number of successes (three times head)</p></li>
<li><p><span class="math notranslate nohighlight">\(n = 3\)</span>: number of trials</p></li>
<li><p><span class="math notranslate nohighlight">\(p = 0.5\)</span>: the probability of getting a head when tossing a coin</p></li>
</ul>
<p>The required probability will be given by:</p>
<div class="math notranslate nohighlight">
\[P(3) = \binom{3}{3}{0.5}^{3}(1 - 0.5)^{3 - 3} = {0.5}^{3} = 0.125\]</div>
<p><strong>Exercise</strong></p>
<p>What is the probability of tossing an unfair coin
(<span class="math notranslate nohighlight">\(P\left( 'head^{'} \right) = 0.6\)</span>) 7 times and obtaining <span class="math notranslate nohighlight">\(2\)</span> tails?</p>
</section>
<section id="categorical-distribution">
<h2>Categorical Distribution<a class="headerlink" href="#categorical-distribution" title="Permalink to this heading">#</a></h2>
<p>The <strong>multinoulli</strong> or <strong>categorical</strong> distribution is a distribution of
a <em>single discrete variable with</em> <span class="math notranslate nohighlight">\(k\)</span> <em>different states</em>, where <span class="math notranslate nohighlight">\(k\)</span> is
finite.</p>
<ul class="simple">
<li><p>The distribution is parametrized by a vector
<span class="math notranslate nohighlight">\(\mathbf{p} \in \lbrack 0,1\rbrack^{k - 1}\)</span>, where <span class="math notranslate nohighlight">\(p_{i}\)</span> gives the
probability of the i^th^ state.</p></li>
<li><p>The probability of the k^th^ state is given by
<span class="math notranslate nohighlight">\(1 - \sum_{i = 1}^{k - 1}{p_{i}\ }\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{p}\)</span> must be such that <span class="math notranslate nohighlight">\(\sum_{i = 1}^{k - 1}p_{i} \leq 1\)</span> to
obtain a valid probability distribution.</p></li>
<li><p>The analytical form of the distribution is given by:
<span class="math notranslate nohighlight">\(p(x = i) = p_{i}\)</span>;</p></li>
</ul>
<p>This distribution is the <strong>generalization of the Bernoulli distribution
to the case of multiple states</strong>.</p>
<p><strong>Example:</strong></p>
<p>Rolling a fair die. In this case, <span class="math notranslate nohighlight">\(k = 6\)</span> and
<span class="math notranslate nohighlight">\(p_{i} = \frac{1}{k}\ ,\ i = 0,\ldots,k - 1\ \)</span>.</p>
</section>
<section id="multinomial-distribution">
<h2>Multinomial Distribution<a class="headerlink" href="#multinomial-distribution" title="Permalink to this heading">#</a></h2>
<p>The multinomial distribution <strong>generalizes the binomial distribution to
the case in which the experiments are not binary</strong>, but they can have
multiple outcomes (e.g., <em>a dice vs a coin</em>).</p>
<p>In particular, the multinomial distribution models the probability of
obtaining exactly <span class="math notranslate nohighlight">\((n_{1},\ldots,n_{k})\)</span> occurrences (with
<span class="math notranslate nohighlight">\(n = \sum_{i}^{}n_{i}\)</span>) for each of the <span class="math notranslate nohighlight">\(k\)</span> possible outcomes in a
sequence of <span class="math notranslate nohighlight">\(n\)</span> independent experiments which follow a Categorial
distribution with probabilities <span class="math notranslate nohighlight">\(p_{1},\ldots,p_{k}\)</span>.</p>
<p>The parameters of the distribution are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span>: the number of trials</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span>: the number of possible outcomes</p></li>
<li><p><span class="math notranslate nohighlight">\(p_{1},\ldots,p_{k}\)</span> the probabilities of obtaining a given class in
each trial (with <span class="math notranslate nohighlight">\(\sum_{i = 1}^{k}p_{i} = 1\)</span>)</p></li>
</ul>
<p>The PMF of the distribution is:</p>
<div class="math notranslate nohighlight">
\[P\left( n_{1},\ldots,n_{k} \right) = \frac{n!}{n_{1}!\ldots n_{k}!}p_{1}^{n_{1}} \cdot \ldots \cdot p_{k}^{n_{k}}\]</div>
<p>The mean is: <span class="math notranslate nohighlight">\(E\left\lbrack n_{i} \right\rbrack = np_{i}\)</span>.</p>
<p>The variance is:
<span class="math notranslate nohighlight">\(Var\left\lbrack n_{i} \right\rbrack = np_{i}(1 - p_{i})\)</span>.</p>
<p>The covariance between two of the input variables is:
<span class="math notranslate nohighlight">\(Cov\left( n_{i},n_{j} \right) = - np_{i}p_{j}\ (i \neq j)\)</span>.</p>
<p><strong>Example</strong></p>
<p>Given a fair die with 6 possible outcomes, what is the probability of
getting 3 times 1, 2 times 2, 4 time 3, 5 times 4, 0 times 5, and 1 time
6, rolling the dice for 15 times?</p>
<p>We have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n = 15\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k = 6\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p_{1} = p_{2} = \ldots p_{6} = \frac{1}{6}\)</span></p></li>
</ul>
<p>The required probability is given by:</p>
<div class="math notranslate nohighlight">
\[P(3,2,4,5,0,1) = \frac{15!}{3!2!4!5!0!1!} \cdot \frac{1}{6^{3}} \cdot \frac{1}{6^{2}} \cdot \frac{1}{6^{4}} \cdot \frac{1}{6^{5}} \cdot \frac{1}{6^{0}} \cdot \frac{1}{6^{1}} = 8.04 \cdot 10^{- 5}\]</div>
</section>
<section id="gaussian-distribution">
<h2>Gaussian Distribution<a class="headerlink" href="#gaussian-distribution" title="Permalink to this heading">#</a></h2>
<p>The Bernoulli and Categorical distributions are PMF, i.e., distributions
over discrete random variables.</p>
<p>A common PDF when dealing with real values is the <strong>Gaussian
distribution,</strong> also known as <strong>Normal Distribution</strong>.</p>
<p>The distribution is characterized by two parameters:</p>
<ul class="simple">
<li><p>The mean <span class="math notranslate nohighlight">\(\mu\mathfrak{\in R}\)</span></p></li>
<li><p>The standard deviation <span class="math notranslate nohighlight">\(\sigma \in (0, + \infty)\)</span></p></li>
</ul>
<p>In practice, the distribution is often <em>seen in terms of</em> <span class="math notranslate nohighlight">\(\mu\)</span> <em>and</em>
<span class="math notranslate nohighlight">\(\sigma^{2}\)</span> rather than <span class="math notranslate nohighlight">\(\sigma\)</span>, where <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is called <strong>the
variance</strong>.</p>
<p>The analytical formulation of the Normal distribution is as follows:</p>
<div class="math notranslate nohighlight">
\[N\left( x;\mu,\sigma^{2} \right) = \sqrt{\frac{1}{2\pi\sigma^{2}}}e^{- \frac{1}{2\sigma^{2}}(x - \mu)^{2}}\]</div>
<p>The term under the square root is a normalization term which ensures
that the distribution integrates to 1.</p>
<p>The expectation and variance of a variable following the Normal
distribution are as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\lbrack x\rbrack = \mu\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Var\lbrack x\rbrack = \sigma^{2}\)</span></p></li>
</ul>
<p>The Gaussian distribution is very used when we do not have much prior
knowledge on the real distribution we wish to model. This in mainly due
to the <strong>central limit theorem</strong>, which states that the sum of many
independent random variables with the same distribution is approximately
normally distributed.</p>
<section id="interpretation">
<h3>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this heading">#</a></h3>
<p><img alt="" src="lectures/lecture5/media/image5.png" />{width=”6.6930555555555555in”
height=”3.3583333333333334in”}</p>
<p>If we plot the PDF of a Normal distribution, we can find that it is easy
to interpret the meaning of its parameters:</p>
<ul class="simple">
<li><p>The resulting curve has a maximum (highest probability) when
<span class="math notranslate nohighlight">\(x = \mu\)</span></p></li>
<li><p>The curve is symmetric, with the inflection points at
<span class="math notranslate nohighlight">\(x = \mu \pm \sigma\)</span></p></li>
<li><p>The example shows a normal distribution for <span class="math notranslate nohighlight">\(\mu = 0\)</span> and
<span class="math notranslate nohighlight">\(\sigma = 1\)</span></p></li>
</ul>
<p><img alt="Risultati immagini per gaussiandistribution" src="lectures/lecture5/media/image6.gif" />{width=”4.9540266841644796in”
height=”3.361111111111111in”}</p>
<p>Another notable property of the Normal distribution is that:</p>
<ul class="simple">
<li><p>About <span class="math notranslate nohighlight">\(68\%\)</span> of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - \sigma,\sigma\rbrack\)</span>;</p></li>
<li><p>About <span class="math notranslate nohighlight">\(95\%\)</span> of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - 2\sigma,2\sigma\rbrack\)</span>;</p></li>
<li><p>Almost 100% of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - 3\sigma,3\sigma\rbrack\)</span>.</p></li>
</ul>
</section>
<section id="multivariate-gaussian">
<h3>Multivariate Gaussian<a class="headerlink" href="#multivariate-gaussian" title="Permalink to this heading">#</a></h3>
<p>The formulation of the Gaussian distribution generalizes to the
multivariate case, i.e., the case in which <span class="math notranslate nohighlight">\(X\)</span> is n-dimensional.</p>
<p>In that case, the distribution is parametrized by a <strong>n-dimensional
vector</strong> <span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> and a
<span class="math notranslate nohighlight">\(\mathbf{n}\mathbf{\times}\mathbf{n}\mathbf{\ }\)</span><strong>positive definite
symmetric matrix</strong> <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>. The formulation of the
multi-variate Gaussian is:</p>
<div class="math notranslate nohighlight">
\[N\left( \mathbf{x;\mu,}\mathbf{\Sigma} \right) = \sqrt{\frac{1}{(2\pi)^{n}\det(\Sigma)}}e^{( - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\Sigma^{- 1}\left( \mathbf{x} - \mathbf{\mu} \right))}\]</div>
<p>Examples of bivariate Gaussian distributions are shown below.</p>
<p><img alt="" src="lectures/lecture5/media/image7.png" />{width=”6.508333333333334in”
height=”2.3983234908136484in”}<img alt="" src="lectures/lecture5/media/image8.png" />{width=”6.508333333333334in”
height=”2.3983234908136484in”}In the 2D case, <span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> is a 2D
point representing the center of the Gaussian (the position of the
mode), whereas the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> influences the “shape” of the
Gaussian.</p>
</section>
<section id="estimation-of-the-parameters-of-a-gaussian-distribution">
<h3>Estimation of the Parameters of a Gaussian Distribution<a class="headerlink" href="#estimation-of-the-parameters-of-a-gaussian-distribution" title="Permalink to this heading">#</a></h3>
<p>We have noted that in many cases we can assume a random variable follows
a Gaussian distribution. However, it is not yet clear how to choose the
parameters of the Gaussian distribution.</p>
<p>Given some data (remember, data is values assumed by random variables!),
we can obtain the parameters of the Gaussian distribution related to the
data with a <strong>maximum likelihood</strong> estimation.</p>
<p>This consists in computing the mean and variance parameters using the
following formula (in the univariate case):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu = \frac{1}{n}\sum_{j}^{}x_{j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^{2} = \frac{1}{n}\sum_{j}^{}\left( x_{j} - \mu \right)^{2}\)</span></p></li>
</ul>
<p>Where <span class="math notranslate nohighlight">\(x_{j}\)</span> represent the different data points.</p>
<p>In the multi-variate case, the computation of the multi-dimensional
<span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> vector is similar:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{\mu} = \frac{1}{n}\sum_{j}^{}\mathbf{x}_{j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is instead computed as the covariance matrix related to
<span class="math notranslate nohighlight">\(X\)</span>: <span class="math notranslate nohighlight">\(\Sigma = Cov(\mathbf{X})\)</span>, i.e.,
<span class="math notranslate nohighlight">\(\Sigma_{ij} = Cov\left( \mathbf{X}_{i},\mathbf{X}_{j} \right)\)</span></p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="information-theory">
<h1>Information Theory<a class="headerlink" href="#information-theory" title="Permalink to this heading">#</a></h1>
<p>Probability Theory allows to deal with the uncertain, however, it does
not allow to quantify it. Information theory provides the tools to
<strong>quantify how much information is present in a signal</strong>.</p>
<p>When dealing with data, it can be useful to understand which data is
informative and which is not, with respect to a given goal.</p>
<p>The basic intuition behind information theory is that learning an
unlikely event has occurred is more informative than learning a likely
event has occurred.</p>
<p>For instance, the event “the sun rose this morning” is very likely and
hence it is not very informative. The message “there was a solar eclipse
this morning” is instead very unlikely, and hence informative.</p>
<section id="self-information">
<h2>Self-information<a class="headerlink" href="#self-information" title="Permalink to this heading">#</a></h2>
<p>We would like to quantify information in a way that formalizes the
intuition discussed before:</p>
<ul class="simple">
<li><p>Likely events should have <em>low information content</em></p></li>
<li><p>Less likely events should have <em>higher information content</em></p>
<ol class="arabic simple">
<li><p>Moreover, <strong>independent events should have additive
information</strong>. For example, finding out that a tossed coin has
come up as heads twice should convey twice as much information
as finding out that a tossed coin has come up as heads once.</p></li>
</ol>
</li>
</ul>
<p>To satisfy these three properties, the notion of <strong>self-information</strong> is
introduced. Given a random variable <span class="math notranslate nohighlight">\(X \sim P\)</span>, the self-information of
an event <span class="math notranslate nohighlight">\(X = x\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[I(x) = - logP(x)\]</div>
<p>If the base of the logarithm is 2, then the self-information is measured
in <strong>bits</strong>.</p>
<p>If the base of the logarithm is e, then the self-information is measures
in <strong>nats.</strong></p>
<p><img alt="" src="lectures/lecture5/media/image11.png" />{width=”4.379285870516186in” height=”3.0625in”}</p>
<p>It should be noted that this definition of self-information satisfies
the three conditions. Indeed:</p>
<ul class="simple">
<li><p>As <span class="math notranslate nohighlight">\(P(x)\)</span> gets larger, <span class="math notranslate nohighlight">\(I(x)\)</span> gets smaller and vice versa</p></li>
<li><p>The self-information for two independent variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is
given by:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(X = x,Y = y) = - \log P(X = x,Y = y)\]</div>
<ul class="simple">
<li><p>Since the variables are independent, <span class="math notranslate nohighlight">\(P(X,Y) = P(X)P(Y)\)</span>, hence:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(X = x,Y = y) = - \log\left\lbrack P(X = x)P(Y = y) \right\rbrack = - logP(X = x) - \log P(Y = y) = I(x) + I(y)\]</div>
<ul class="simple">
<li><p>Hence, also the additivity of the entropies of independent variables
is satisfied.</p></li>
</ul>
</section>
<section id="entropy">
<h2>Entropy<a class="headerlink" href="#entropy" title="Permalink to this heading">#</a></h2>
<p><em>Self-information is defined on a single outcome</em>. We can quantify the
amount of <strong>uncertainty</strong> in an entire probability distribution using
the <strong>Shannon entropy</strong>:</p>
<div class="math notranslate nohighlight">
\[H(X) = E_{X \sim P}\left\lbrack I(x) \right\rbrack = - E_{X \sim P}\left\lbrack \log{P(x)} \right\rbrack\]</div>
<p>In the case of a discrete variable:</p>
<div class="math notranslate nohighlight">
\[H(X) = - \sum_{x}^{}{P(x)logP(x)}\]</div>
<p>Where the summation is over all values of <span class="math notranslate nohighlight">\(x \in \ \chi\)</span>, with <span class="math notranslate nohighlight">\(\chi\)</span>
alphabet of <span class="math notranslate nohighlight">\(X\)</span>. In the case of a continuous variable, the sum is
substituted by an integral:</p>
<div class="math notranslate nohighlight">
\[H(X) = - \int_{}^{}{P(x)logP(x)dx}\]</div>
<p>The Shannon entropy of a distribution is <strong>the expected amount of
information in an event drawn from that distribution</strong>.</p>
<p>The entropy is measured in <strong>bits</strong> or <strong>nats</strong> (depending on the base
of the logarithm) and can be interpreted as <strong>the amount of information
required on the average to describe the random variable</strong>.</p>
<p><strong>Example</strong></p>
<ul class="simple">
<li><p>Let’s consider a die with six faces.</p></li>
<li><p>We can describe the outcomes of rolling the dice with a random
variable <span class="math notranslate nohighlight">\(X\)</span> with alphabet <span class="math notranslate nohighlight">\(\chi = \ \left\{ 1,\ldots,6 \right\}\)</span>.</p></li>
<li><p>If the dice is fair, then <span class="math notranslate nohighlight">\(P(x) = \frac{1}{6},\ \forall x \in \chi\)</span>.</p></li>
<li><p>The entropy associated with this random variable is:
<span class="math notranslate nohighlight">\(H(X) = - \sum_{i = 1}^{6}{\frac{1}{6}\log}\frac{1}{6} = \log(6) = 2.58\ bits\)</span>.</p>
<ol class="arabic simple">
<li><p>This makes sense as the minimum number of bits needed to
represent 8 numbers is 3 (<span class="math notranslate nohighlight">\(2^{3} = 8\)</span>). Since we have less than
8 numbers, in average we need less than 3 bits.</p></li>
</ol>
</li>
</ul>
<p> </p>
<ul class="simple">
<li><p>Let’s suppose the dice is not fair and in particular Let’s suppose
it follows a categorical distribution with
<span class="math notranslate nohighlight">\(\mathbf{p} = \lbrack 0.1,0.1,0.1,0.1,0.1\rbrack\)</span>. The probability
of number “6” will be <span class="math notranslate nohighlight">\(0.5\)</span>;</p></li>
<li><p>Intuitively, we need less bits to represent this variable. Indeed,
since event <span class="math notranslate nohighlight">\(X = 6\)</span> is much more frequent, we can represent it with
the least number of bits, whereas we can represent less frequent
outcomes with more bits. In particular, the entropy of the variable
will be:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[H(X) = - \sum_{i = 1}^{5}{0.1\log{0.1}} - 0.5\log(0.5) = 2.16\ bits\]</div>
<ul class="simple">
<li><p>The number of bits required to represent the variable in average is
2.16 bits, smaller than in the case of the fair die.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="questions">
<h1>Questions<a class="headerlink" href="#questions" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>What is data?</p></li>
<li><p>Is there a difference between data and information?</p></li>
<li><p>What is the difference between a variable and a random variable?</p></li>
<li><p>What is the goal of probability theory?</p></li>
<li><p>What is the difference between independence and conditional
independence among random variables?</p></li>
<li><p>What are the units of measurement of information?</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>Parts of chapter 1 of [1];</p></li>
<li><p>Most of chapter 3 of [2];</p></li>
<li><p>Some parts of chapter 1 of [3].</p></li>
</ul>
<p>[1] Bishop, Christopher M. <em>Pattern recognition and machine learning</em>.
springer, 2006.
<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
<p>[2] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep
learning</em>. MIT press, 2016. <a class="reference external" href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
<p>[3] Cover, Thomas M., and Joy A. Thomas. <em>Elements of information
theory</em>. John Wiley &amp; Sons, 2012.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/lecture5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Probability Distributions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmf-discrete-variables">Probability Mass Functions (PMF) – Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-fair-coin">Example: Probability Mass Function for a Fair Coin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-skewed-coin">Example: Probability Mass Function for a Skewed Coin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-probability-mass-function">Exercise: Probability Mass Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdf-continuous-variables">Probability Density Functions (PDF) – Continuous Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-uniform-pdf">Example: Uniform PDF</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-distributions">Common Probability Distributions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">Bernoulli Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">Binomial Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">Categorical Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-distribution">Multinomial Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution">Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">Multivariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-gaussian-distribution">Estimation of the Parameters of a Gaussian Distribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theory">Information Theory</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-information">Self-information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#questions">Questions</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>